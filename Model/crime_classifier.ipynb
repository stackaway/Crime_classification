{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import calendar\n",
    "import nltk \n",
    "import keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING ALL ORIGINAL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGRESSION PARSED DATASET\n",
    "ori_aggression_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/aggression_parsed_dataset.csv'\n",
    "ori_aggression = pd.read_csv(ori_aggression_dataset)\n",
    "ori_aggression = ori_aggression.dropna()\n",
    "ori_aggression.columns\n",
    "ori_aggression = ori_aggression.drop(columns=['index','ed_label_0','ed_label_1', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE DATASET\n",
    "\n",
    "ori_kaggle_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/kaggle_parsed_dataset.csv'\n",
    "ori_kaggle = pd.read_csv(ori_kaggle_dataset)\n",
    "ori_kaggle= ori_kaggle.dropna()\n",
    "ori_kaggle.columns\n",
    "ori_kaggle = ori_kaggle.drop(columns=['index','oh_label', 'Date',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOXICITY PARSED DATASET\n",
    "\n",
    "ori_toxicity_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/toxicity_parsed_dataset.csv'\n",
    "ori_toxicity = pd.read_csv(ori_toxicity_dataset)\n",
    "ori_toxicity = ori_toxicity.dropna()\n",
    "ori_toxicity.columns\n",
    "ori_toxicity = ori_toxicity.drop(columns=['index','ed_label_0', 'ed_label_1', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWITTER PARSED DATASET\n",
    "\n",
    "ori_twitter_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/twitter_parsed_dataset.csv'\n",
    "ori_twitter = pd.read_csv(ori_twitter_dataset)\n",
    "ori_twitter = ori_twitter.dropna()\n",
    "ori_twitter.columns\n",
    "ori_twitter = ori_twitter.drop(columns=['index','id','Annotation', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER RACISM DATASET\n",
    "\n",
    "ori_twit_racist_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/twitter_racism_parsed_dataset.csv'\n",
    "ori_twit_racist = pd.read_csv(ori_twit_racist_dataset)\n",
    "ori_twit_racist = ori_twit_racist.dropna()\n",
    "ori_twit_racist.columns\n",
    "ori_twit_racist = ori_twit_racist.drop(columns = ['index', 'id','Annotation', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER SEXISM DATASET\n",
    "\n",
    "ori_twit_sexism_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/twitter_sexism_parsed_dataset.csv'\n",
    "ori_twit_sexism = pd.read_csv(ori_twit_sexism_dataset)\n",
    "ori_twit_sexism = ori_twit_sexism.dropna()\n",
    "ori_twit_sexism.columns\n",
    "ori_twit_sexism = ori_twit_sexism.drop(columns=['index', 'id', 'Annotation', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUTUBE PARSED DATASET\n",
    "\n",
    "ori_utube_dataset = '/Users/churnika/Desktop/Projects/Crime_classification/dataset/youtube_parsed_dataset.csv'\n",
    "ori_utube = pd.read_csv(ori_utube_dataset)\n",
    "ori_utube = ori_utube.dropna()\n",
    "ori_utube.columns\n",
    "ori_utube = ori_utube.drop(columns=['index', 'UserIndex','Number of Comments','Number of Subscribers', 'Membership Duration', 'Number of Uploads','Profanity in UserID', 'Age', 'oh_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [ori_aggression, ori_kaggle, ori_toxicity, ori_twitter, ori_twit_racist, ori_twit_sexism]\n",
    "text = pd.concat(frames)\n",
    "text.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328304\n"
     ]
    }
   ],
   "source": [
    "msg_exp = text['Text'].str.lower()\n",
    "print(len(msg_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "msg_exp = msg_exp.apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "msg_exp = msg_exp.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/churnika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/churnika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define the words to be removed\n",
    "months_days = [calendar.month_name[i].lower() for i in range(1, 13)] + [calendar.day_name[i].lower() for i in range(7)]\n",
    "remove_words = [\"vo\",\"n\",\"m\",\"c\",\"ra\",\"xx\",\"r\",\"date\",\"hii\",\"hi\",\"ye\",\"pa\",\"xxx\",\"p\",\"sir\",\"mam\",\"good\",\"morning\",\"time\",\"ur\",\"you\",\"status\",\"father\"]\n",
    "\n",
    "# Define a function to remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\\\S+|www\\\\.\\\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "# Define a function to tokenize the text and remove unwanted words\n",
    "def process_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if not re.match(r'http[s]?://', token)]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token not in months_days]\n",
    "    tokens = [token for token in tokens if token not in remove_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to remove URLs\n",
    "msg_exp = msg_exp.apply(remove_urls)\n",
    "\n",
    "# Apply the function to process the text\n",
    "msg_exp = msg_exp.apply(process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328304\n"
     ]
    }
   ],
   "source": [
    "print(len(msg_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     msg_exp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(msg_exp)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert to TensorFlow tensor\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m msg_exp_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(msg_exp)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load your models\u001b[39;00m\n\u001b[1;32m     16\u001b[0m racism_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mracism_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/my_project_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/my_project_env/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "# Assuming msg_exp is your input data\n",
    "msg_exp = np.array(msg_exp)\n",
    "\n",
    "# Convert msg_exp to a NumPy array if it's not already\n",
    "if not isinstance(msg_exp, np.ndarray):\n",
    "    msg_exp = np.array(msg_exp)\n",
    "\n",
    "# Convert to TensorFlow tensor\n",
    "msg_exp_tensor = tf.convert_to_tensor(msg_exp)\n",
    "\n",
    "# Load your models\n",
    "racism_model = load_model('racism_model.h5')\n",
    "sexism_model = load_model('sexism_model.h5')\n",
    "\n",
    "# Use your models to predict\n",
    "racism_preds = racism_model.predict(msg_exp_tensor)\n",
    "sexism_preds = sexism_model.predict(msg_exp_tensor)\n",
    "\n",
    "# Perform further processing as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
